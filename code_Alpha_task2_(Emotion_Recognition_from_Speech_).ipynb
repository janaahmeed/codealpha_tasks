{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janaahmeed/codealpha_tasks/blob/main/code_Alpha_task2_(Emotion_Recognition_from_Speech_).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "P8UTA402oj4B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvTKsVo9wNr"
      },
      "source": [
        "After framing, each frame is multiplied by a Hamming window to reduce spectral leakage before silence detection and further feature extraction.  \n",
        "# When you use librosa.feature.mfcc, framing + windowing (Hamming) are already applied internally by default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fkKh1QKCnqF"
      },
      "source": [
        "y>> the audio time-series signal , Compute MFCC features from audio signal lf sampled at fs Hz\n",
        "y(t) = audio_signal\n",
        "mfcc(y=audio_signal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRI2SDUd3lTn"
      },
      "source": [
        "Always join files with root, never dirs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aVYBVB_SkriS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "import librosa\n",
        "import numpy as np\n",
        "def apply_random_augmentation(signal, sr):\n",
        "    augmentations = [\"none\", \"stretch\", \"shift\", \"pitch\", \"noise\"]\n",
        "    choice = random.choice(augmentations)\n",
        "\n",
        "    noise_factor = 0.005\n",
        "\n",
        "    if choice == \"noise\":\n",
        "        noise_factor = 0.005\n",
        "        signal = signal + np.random.randn(len(signal)) * noise_factor\n",
        "\n",
        "    elif choice == \"stretch\":\n",
        "        rate = np.random.uniform(0.8, 1.2)\n",
        "        signal = librosa.effects.time_stretch(signal, rate=rate)\n",
        "\n",
        "    elif choice == \"shift\":\n",
        "        shift = int(np.random.uniform(-0.2, 0.2) * len(signal))\n",
        "        signal = np.roll(signal, shift)\n",
        "\n",
        "    elif choice == \"pitch\":\n",
        "        signal = librosa.effects.pitch_shift(\n",
        "            signal,\n",
        "            sr=sr,\n",
        "            n_steps=np.random.uniform(-2, 2)\n",
        "        )\n",
        "\n",
        "    return signal\n",
        "\n",
        "\n",
        "\n",
        "def load_audio(path, sr=44100):\n",
        "    signal, sr = librosa.load(path, sr=sr, offset=0.5, duration=2.5)\n",
        "\n",
        "    signal = apply_random_augmentation(signal, sr)\n",
        "\n",
        "    return signal, sr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGHFTjW8qH_U",
        "outputId": "e4a7570e-3f81-41c4-83f7-5fac0c5a6c91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cached features...\n",
            "Features saved.\n",
            "(8640, 251, 128, 1) (8640,)\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "import os\n",
        "import kagglehub\n",
        "\n",
        "DATA_FILE = \"ravdess-emotional-speech-audio\"\n",
        "\n",
        "if os.path.exists(DATA_FILE):\n",
        "    print(\"Loading cached features...\")\n",
        "\n",
        "    x, y ,speakers = joblib.load(DATA_FILE)\n",
        "\n",
        "else:\n",
        "    print(\"Processing images (one-time)...\")\n",
        "\n",
        "# Download latest version\n",
        "    path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "\n",
        "\n",
        "    print(\"Path to dataset files:\", path)\n",
        "    x,y, speakers=[],[],[]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for _file in files:\n",
        "        if _file.endswith(\".wav\"):\n",
        "            file_path = os.path.join(root, _file)\n",
        "            aug_data ,sr  =load_audio(file_path )\n",
        "\n",
        "            y_trimmed, _ = librosa.effects.trim(aug_data ,top_db=25)  # 20–30 is common for speech\n",
        "\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "            y=y_trimmed,\n",
        "            sr=sr,\n",
        "            n_fft=2048,\n",
        "            hop_length=512,\n",
        "            n_mels=128\n",
        "                    )\n",
        "\n",
        "        # 3. Convert power to decibels (log scale)\n",
        "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        # 4. Transpose if you are using LSTMs (Time steps first)\n",
        "        # Resulting shape will be (251, 128) based on your target\n",
        "        mel_spec_db = mel_spec_db.T\n",
        "\n",
        "        x.append(mel_spec_db)\n",
        "\n",
        "        # Extract label (assuming RAVDESS-style naming: 03-01-01-...)\n",
        "        emo_str = _file.split(\"-\")[2]\n",
        "        y.append(int(emo_str))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "joblib.dump((x, y ,speakers), DATA_FILE)\n",
        "print(\"Features saved.\")\n",
        "#(n_mfcc, time_frames) >>shape mfcc\n",
        "# l your mel_spec_db arrays have the same first dimension (128) But different time steps (216, 210, 198, etc.)\n",
        "#solution to avoid : (n_samples, 128, ?)\n",
        " #This forces NumPy to:create a 1D object array first ,Then insert each 2D MFCC/mel matrix as a separate object ,No broadcasting attempt happens\n",
        "\n",
        "\n",
        "max_len = 251\n",
        "padded_x = []\n",
        "\n",
        "for sample in x:\n",
        "    # sample shape: (time, 128)\n",
        "\n",
        "    if sample.shape[0] < max_len:\n",
        "        pad_width = max_len - sample.shape[0]\n",
        "        sample = np.pad(\n",
        "            sample,\n",
        "            pad_width=((0, pad_width), (0, 0)),\n",
        "            mode='constant'\n",
        "        )\n",
        "    else:\n",
        "        sample = sample[:max_len, :]\n",
        "\n",
        "    padded_x.append(sample)\n",
        "\n",
        "# NOW convert safely\n",
        "x = np.array(padded_x, dtype='float32')\n",
        "x=x[..., np.newaxis]\n",
        "\n",
        "y = np.array(y)\n",
        "\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "G2icQ2_Tzof5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b027c3c-0305-47f8-d066-01981904c166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (8640, 251, 128, 1)\n",
            "y shape: (8640,)\n",
            "Unique labels: [1 2 3 4 5 6 7 8]\n",
            "Random baseline: 0.125\n"
          ]
        }
      ],
      "source": [
        "print(\"X shape:\", x.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"Unique labels:\", np.unique(y))\n",
        "print(\"Random baseline:\", 1 / len(np.unique(y)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eag1__p2v8-k"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(np.unique(y))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "eD8n9uIL2PUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3508b229-a6d3-4b0c-9cbd-2bd1ae7f691e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8640, 251, 128, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV9qKvGaFgvp"
      },
      "source": [
        "//CNN / LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_NgzhFJgJwEa"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "#y_encoded = to_categorical(y_encoded ,num_classes=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "itYuWWoAPjg8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    x,y_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9_wacgGEtan"
      },
      "source": [
        "MFCC features have limited frequency resolution, so aggressive pooling across both axes may collapse feature maps.\n",
        "We therefore apply pooling mainly along the time axis and preserve frequency information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27hp1EiH8pj5"
      },
      "source": [
        "model.add(Reshape((-1, x.shape[2] * 128)))\n",
        "Why?\n",
        "\n",
        "-1 lets Keras infer time steps automatically\n",
        "\n",
        "Works even if you change pooling later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LrOkL0rGCpk"
      },
      "source": [
        "model.add(MaxPooling2D(pool_size=(1, 2)))\n",
        "This:\n",
        "\n",
        "reduces noise\n",
        "\n",
        "keeps emotion dynamics\n",
        "\n",
        "stabilizes LSTM learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrWkLLHeH5Qd"
      },
      "source": [
        "Frequency pooling:\n",
        "\n",
        "smooths spectral noise\n",
        "\n",
        "improves generalization\n",
        "\n",
        "stabilizes training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrUzzahxEms3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, ReLU, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# Conv → BN → ReLU → Pool.\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer: 128 Mel bands x 251 Time bins x 1 Grayscale channel\n",
        "model.add(Input(shape=(128, 251, 1)))\n",
        "\n",
        "# --- CNN Block 1 ---\n",
        "# 32 filters, 5x5 kernel, stride (5,3)>> image\n",
        "model.add(Conv2D(32, (5, 5), strides=(5, 3), padding='valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(ReLU())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# --- CNN Block 2 ---\n",
        "# 64 filters, 3x3 kernel, stride (3,2)\n",
        "model.add(Conv2D(64, (3, 3), strides=(3, 2), padding='valid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(ReLU())\n",
        "\n",
        "# --- CNN Block 3 ---\n",
        "# 128 filters, 3x3 kernel, 'same' padding\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(ReLU())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# --- Transition to Classifier ---\n",
        "model.add(Flatten())\n",
        "\n",
        "# --- Dense Layer 1 ---\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(ReLU())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# --- Dense Layer 2 ---\n",
        "model.add(Dense(128))\n",
        "model.add(BatchNormalization())\n",
        "model.add(ReLU())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# --- Output Layer ---\n",
        "\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "# Compilation\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer='Adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Prepare for LSTM >> flattten inseadreshape (frq * channels )\n",
        "#model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "# LSTM blocks\n",
        "#model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "\n",
        "#model.add(Dropout(0.3))\n",
        "\n",
        "#model.add(Bidirectional(LSTM(256, return_sequences=False)))\n",
        "\n",
        "#model.add(Dropout(0.3))\n",
        "\n",
        "#model.add(GlobalAveragePooling1D())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, ReLU, Flatten, Dense, Input\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, ReLU, Flatten, Dense, Input\n",
        "\n",
        "inputs = Input(shape=(251, 128, 1))\n",
        "\n",
        "x_layer = Conv2D(32, (5,5), strides=(5,3), padding='valid')(inputs)\n",
        "x_layer = BatchNormalization()(x_layer)\n",
        "x_layer = ReLU()(x_layer)\n",
        "x_layer = MaxPooling2D((2,2))(x_layer)\n",
        "\n",
        "x_layer = Conv2D(64, (3,3), strides=(3,2), padding='valid')(x_layer)\n",
        "x_layer = BatchNormalization()(x_layer)\n",
        "x_layer = ReLU()(x_layer)\n",
        "\n",
        "x_layer = Conv2D(128, (3,3), padding='same')(x_layer)\n",
        "x_layer = BatchNormalization()(x_layer)\n",
        "x_layer = ReLU()(x_layer)\n",
        "x_layer = MaxPooling2D((2,2))(x_layer)\n",
        "\n",
        "flatten_output = Flatten(name='flatten_layer')(x_layer)\n",
        "\n",
        "dense = Dense(256, activation='relu')(flatten_output)\n",
        "dense = Dense(128, activation='relu')(dense)\n",
        "outputs = Dense(8, activation='softmax')(dense)\n",
        "\n",
        "#model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Feature extractor\n",
        "feature_extractor = Model(inputs=inputs, outputs=flatten_output)\n",
        "\n",
        "# 2. Define the feature extractor\n",
        "# We map the model's input to the output of the 'flatten_layer'\n",
        "feature_extractor = Model(\n",
        "    inputs=model.input,\n",
        "    outputs=model.get_layer('flatten_layer').output\n",
        ")\n",
        "# 3. Predict to get the features (ensure x is shape (samples, 128, 251, 1))\n",
        "X_features = feature_extractor.predict(x)\n",
        "\n",
        "# 4. Feature Selection (Picking top 100 features)\n",
        "selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
        "X_selected = selector.fit_transform(X_features, y)\n",
        "\n",
        "# 5. Scaling (Essential for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_selected)\n",
        "\n",
        "# 6. Initialize and Train SVM\n",
        "svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)\n",
        "svm_classifier.fit(X_scaled, y)\n",
        "\n",
        "# 7. Results\n",
        "accuracy = svm_classifier.score(X_scaled, y)\n",
        "print(f\"SVM Classification Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "QR5EPqxpVCsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734ce231-130c-44eb-b4cd-26562f662d79"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
            "SVM Classification Accuracy: 71.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1US7uuh-s2c"
      },
      "source": [
        "-(batch, 7) from LSTM\n",
        "-matches output with sparse_categorical_crossentropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vYYOn2hEJWx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d884d3de-ec93-4dee-c86a-31e4b4ea7248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int64\n"
          ]
        }
      ],
      "source": [
        "print(y_test.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3shJEgpeGERt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00dfac01-d9b9-4e36-b62e-80b39dae0d06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB0w68T2GsJ4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNK2HX7-FohJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y),\n",
        "    y=y\n",
        ")\n",
        "\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(class_weights)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}