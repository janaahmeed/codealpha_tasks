{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh3vUf4BJLOBd/9HCft5Ts",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janaahmeed/codealpha_tasks/blob/main/codeAlpha_task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P8UTA402oj4B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RDUKFjCopm4",
        "outputId": "2e1e6e40-8eff-489c-890a-452f38d3bdb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'ravdess-emotional-speech-audio' dataset.\n",
            "Path to dataset files: /kaggle/input/ravdess-emotional-speech-audio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After framing, each frame is multiplied by a Hamming window to reduce spectral leakage before silence detection and further feature extraction.  \n",
        "# When you use librosa.feature.mfcc, framing + windowing (Hamming) are already applied internally by default."
      ],
      "metadata": {
        "id": "vkvTKsVo9wNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "y>> the audio time-series signal , Compute MFCC features from audio signal lf sampled at fs Hz\n",
        "y(t) = audio_signal\n",
        "mfcc(y=audio_signal)"
      ],
      "metadata": {
        "id": "1fkKh1QKCnqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x, y = [], []\n",
        "plt.rcParams['figure.figsize'] = (18,4)\n",
        "\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for _file in files:\n",
        "        if _file.endswith(\".wav\"):\n",
        "            file_path = os.path.join(root, _file)\n",
        "\n",
        "            lf, fs = librosa.load(file_path, duration=0.03)\n",
        "\n",
        "            y_trimmed, _ = librosa.effects.trim(lf ,top_db=25 )  # 20–30 is common for speech\n",
        "\n",
        "            mfccs = librosa.feature.mfcc(y=y_trimmed, sr=fs, window='hamming')\n",
        "            mfccs = mfccs.T # (time_steps, n_mfcc)\n",
        "            # Why? LSTM expects time dimension first\n",
        "\n",
        "            x.append(mfccs)\n",
        "            y.append(_file.split(\"-\")[2])\n",
        "\n",
        "#(n_mfcc, time_frames) >>shape mfcc\n",
        "\n",
        "\n",
        "#NumPy arrays require equal shapes\n",
        "#Your MFCCs are variable-length sequences >> so we neednot ( Pad for CNN / Use sequence models (LSTM) / Use tf.keras.preprocessing.sequence.pad_sequences)\n",
        "#“Don’t force uniform shape — store each MFCC as a separate object” if duration is fixed we donot need for it\n",
        "#so type object\n",
        "x = np.array(x, dtype=object)\n",
        "y = np.array(y)\n",
        "\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGHFTjW8qH_U",
        "outputId": "a78349ce-6ef2-4603-ed6a-9a4359648a4c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=662\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2880, 2, 20) (2880,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "x=tf.keras.preprocessing.sequence.pad_sequences(x ,padding='post')\n",
        "\n",
        "#for conv2d new dimansion to adapt MFCC inputs for convolutional layers.\n",
        "x=x[..., np.newaxis]\n"
      ],
      "metadata": {
        "id": "H6hwlBba6e5_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "//CNN / LSTM"
      ],
      "metadata": {
        "id": "RV9qKvGaFgvp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "itYuWWoAPjg8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}